#!/usr/bin/perl -w


# TODO: add an option to parse the original filename
# TODO: figure out how to reconcile the original filename duplicates
# TODO: fix extraction to catch .gif files

use strict;
use warnings;

use Data::Dumper;
use Getopt::Std;
use LWP::UserAgent;

our $VERSION            = 1.0;
my $FUTABA_AGENT        = undef;
my $FUTABA_TIMEOUT      = 10;
my $FUTABA_PROXY        = 0;
my $FUTABA_UA           = new LWP::UserAgent;
my $FUTABA_SCRAPE_INTV  = 10;
$|++;

BEGIN
{
    sub main
    {
        my $prefs = init();
        exit event_loop($prefs);
    }

    sub usage
    {
        print STDERR <<END_OF_USAGE;
Usage: futaba-thread-get [options] URL1 [URL2] [URL3] [...]
       futaba-thread-get [options] -l URL_LIST_FILE

 -h -?        Prints this help message and exits
 -m           Turns on Url Output Mode. Instead of downloading the images from
              the scraped Urls, they are written to STDOUT.
 -n INTERVAL  Specify the interval (in seconds) between thread scrapes
 -l URL_FILE  Provide a file with one URL per line to process.
 -d DIR       Destination directory for the files. (Defaults to \$PWD)
END_OF_USAGE
        exit 1;
    }

    sub init
    {
        my %prefs;
        my %options;

        getopts("ml:d:n:h?",\%options);
        usage() if exists $options{h} or exists $options{'?'};

        # Setup the LWP::UserAgent instance
        $FUTABA_UA->env_proxy                if $FUTABA_PROXY;
        $FUTABA_UA->timeout($FUTABA_TIMEOUT) if $FUTABA_TIMEOUT;
        $FUTABA_UA->agent($FUTABA_AGENT)     if $FUTABA_AGENT;

        # Setup the scrape interval
        $prefs{interval} = $options{n} ? $options{n} : $FUTABA_SCRAPE_INTV;

        # Setup download command
        $prefs{dl_command} = 'wget -q -O FILE URL';
        `which wget`;
        die "Could not find wget in \$PATH" if $?;

        # Setup the mode to run in
        if(exists $options{m}) {
            $prefs{mode}    = 'out';
            $prefs{verbose} = 0;
        } else {
            $prefs{mode}    = 'proc';
            $prefs{verbose} = 1;
        }

        # Populate the url list
        $prefs{url_list} = [];
        if(exists $options{l}) {
            open FILE, "<$options{l}" or die "Error opening file $options{l}: $!";
            while(<FILE>) {
                push @{$prefs{url_list}}, $_;
            }
        } else {
            map { +push @{$prefs{url_list}}, $_; } @ARGV;
        }
        die "No urls provided" unless scalar @{$prefs{url_list}};

        # Setup the destination directory
        $prefs{dest_dir} = exists $options{d} ? $options{d} : $ENV{PWD};

        return \%prefs;
    }

    # load_thread_list
    #
    # Loads a list of thread urls from a file. (one url per line)
    #
    sub load_thread_list
    {
        my $file = shift;
        open FILE, '<', $file or die "Could not open '$file': $!";
        my @list = <FILE>;
        close FILE or die "Could not close '$file': $!";
        return @list;
    }

    # event_loop()
    #
    # The main loop that continuously runs through each thread url. Returns when
    # there are no more thread urls to process.
    #
    sub event_loop
    {
        my $break_loop = 0;
        my $prefs      = shift;

        while(!$break_loop) {
            last unless scalar @{$prefs->{url_list}};   # no more urls to process

            my $i = 0;
            my @dead_threads;

            for my $thread_url (@{$prefs->{url_list}}) {
                my $result = process_thread($prefs,$thread_url);
                push @dead_threads, $i unless $result;
                $i++;
            }

            prune_thread_list($prefs->{url_list},\@dead_threads,$prefs->{verbose});

            print "Sleeping for $prefs->{interval} seconds.\n" if $prefs->{verbose};
            sleep $prefs->{interval};
        }
        return 0;
    }

    sub prune_thread_list
    {
        my ($thread_list, $dead_list, $verbose) = @_;

        return undef unless scalar @$dead_list;

        my $offset = 0;
        for my $dead_thread_index (@$dead_list) {
            print "Removing thread url '$thread_list->[$dead_thread_index-$offset]'.\n" if $verbose;
            splice @$thread_list, $dead_thread_index-$offset, 1;
            $offset++;
        }

        return 1;
    }

    # processes a single thread url. extracts image urls, then either prints
    # the list or downloads them all depending on the mode
    sub process_thread
    {
        my ($prefs,$thread_url) = @_;

        print "Processing thread url '$thread_url'.\n" if $prefs->{verbose};
        
        my $image_url_list = get_image_url_list($thread_url);
        return undef unless $image_url_list;

        return print_url_list($image_url_list)              if $prefs->{mode} eq 'out';
        return download_url_list($prefs,$image_url_list)    if $prefs->{mode} eq 'proc';
        return undef;
    }

    # prints out url list to stdout
    sub print_url_list
    {
        my $url_list = shift;
        map { +print "$_\n"; } @$url_list;
        return 1;
    }

    sub download_url_list
    {
        my ($prefs,$image_url_list) = @_;
        
        if(scalar @$image_url_list < 1) {
            print "No images. Skipping.\n" if $prefs->{verbose};
            return 1;
        }

        for (@$image_url_list) {
            print "\t$_..." if $prefs->{verbose};
            my $result = download_image_url($_,$prefs->{dl_command},$prefs->{dest_dir},$prefs->{verbose});
            print "$result\n" if $prefs->{verbose};
        }
        
        return 1;
    }

    sub process_7chan_content
    {
        my $page_content = shift;
        my @images  = ();

        # Match things that look like this:
        #   <td class="reply" id="reply4169"> ... </td>
        # Each of these is one post in a thread.
        my @replies = ($page_content =~ m!(<td\ class="reply"\ id="reply[[:digit:]]*">.*?</td>)!smigx);

        for my $reply (@replies) {
            # process 'multithumb' replies
            if($reply =~ m/<span class="multithumb"/) {
                my @imgs = 
                    $reply =~ m!<a\s+
                                target="_blank"\s+
                                href="([^"]+)">.*?  # grab direct link to the image
                                <img.*?
                                title="[^"]+        # start into the title of the image
                                \([^)]+,[^)]+,\     # ignore the first two CSVs between the parens
                                ([^)"]+)            # capture everything after the 2nd comma before the closing paren
                                \)"!msgix;
                # there should be an even number of results
                die "Error 25" if (scalar @imgs) % 2 != 0;
                for (my $i=0; $i<scalar @imgs; ++$i) {
                    push @images, $imgs[$i];
                    $i++; # skip the filename for now
                }
            }
            # process replies with images; the matches look like this:
            #     <a href="https://www.7chan.org/gif/src/125410203227.jpg" onclick="javascript:expandimg('4167', 'https://www.7chan.org/gif/src/125410203227.jpg', 'https://www.7chan.org/gif/thumb/125410203227s.jpg', '982', '1420', '86', '125');return false;">
            #     125410203227.jpg</a>
            #
            #     - (245.19KB
            #
            #     , 982x1420
            #
            #     , filename goes here.gif
            #
            #     )
            #
            #     </span>
            elsif($reply =~ m/<span\s+id="thumb\d+"/) {
                my @pair =
                    $reply =~ m!<a\s+
                                href="([^"]+)"\s+   # grab the direct link to the image
                                onclick="javascript:expandimg.*?</a> # javascript as confirmation of right link
                                .*?
                                \(.*?,.*?,\ 
                                ([^\)]+[^\s])\s+    # grab the filename; make sure last character is not whitespace
                                \)!msgix;
                print "image url = $pair[0]\n";
                print "filename = $pair[1]\n";
            }
        }
    }

    sub get_image_url_list
    {
        my $thread_url    = shift;
        my $http_response = $FUTABA_UA->get($thread_url);
        return undef unless $http_response->is_success;
        my $page_content  = $http_response->content;

        if($thread_url =~ m{(\.|/)7chan\.org}) {
            return process_7chan_content($page_content);
        } else {
            die ( "Should not be here" );
        }
        # Extract image urls from content
        my @matches =
            $http_response->content =~ m!<img\s*.*?\s*src="?([^\s>"]*/thumb/[0-9]*s\.[a-zA-Z]{3})"?!migx;
        s!thumb/([0-9]*)s\.([a-z]{3})!src/$1.$2!i for (@matches);

        # Construct a base url (some sites use relative urls, and we need to
        # construct absolute urls
        $thread_url =~ m{^([a-z]*://[^\s/]*).*$}i;
        my $urlbase = $1;

        # Build the final list (make sure each url is an absolute url before
        # returning a list).
        my @final_list;
        for my $match (@matches)
        {
            $match = "$urlbase$match" if $match =~ m{^/};
            push @final_list, $match;
        }
        return \@final_list;
    }

    sub download_image_url
    {
        my ($image_url,$command,$dest_dir,$verbose) = @_;
        
        my $filename = $image_url;
           $filename =~ s{^.*/([^/]*)$}{$1};

        my $tmp_path  = "$dest_dir/$filename.part";
        my $file_path = "$dest_dir/$filename";

        return "Already Exists" if -f $file_path;
        unlink($tmp_path)       if -f $tmp_path;

        $command =~ s!URL!\'$image_url\'!;
        $command =~ s!FILE!\"$tmp_path\"!;

        system($command);
        return "Failure" if $?;

        rename $tmp_path, $file_path;
        return "Success";
    }
}

main();
